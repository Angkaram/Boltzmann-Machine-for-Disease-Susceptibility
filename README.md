# A Python project applying a Restricted Boltzmann Machine (RBM) structure using [Genomic data](https://www.internationalgenome.org) to create a simple neural network that tracks or predicts disease susceptibility.

**Project Progress:**
1. July 19, 2024: Using the [book](http://www.inference.org.uk/itprnn/book.pdf) *Information Theory, Inference, and Learning Algorithms* by David J.C. MacKay, I am learning about the complex mathematics and logic that goes into Neural Networks (especially with Chapter 40). With this information, I am also solving some of the exercises from the book (42.5 and 42.8 are shown in the  exercises folder in this repository). 
2. July 24, 2024: For exercise 42.5, I edited the visual format to reproduce Figure 42.3 and the DJCM visual. I also ensured that the recall was asynchronous as opposed to synchronous to guarantee the decrease of the Liapunov function. I also added exercise 39.5 (The Noisy LED), which is about calculating the probabilities of different characters being displayed on a noisy 7-segment LED display, given the current state of the display elements and a predefined flipping probability. My output for this exercise does not mimic a 7-segment LED, but perhaps future modifications will change this.
3. July 29, 2024: I start the main project using the prior exercises and the book as a foundation. I begin with researching for Genomic data that is in a format that can easily be handled by a RBM. I find [this](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage_SV/working/20190825_Yale_cnvnator/) from the [1000 Genomes Project](https://www.internationalgenome.org). Once I have the data, I must preprocess it. I do so by "binarizing" the data by setting a threshold by which the data is transformed into a 1 or 0. With this binary format, the data is then ready to be processed by the RBM. I created a RBM class that is trained with a simple Hebbian Learning rule (42.1 in the book). I also have simple metrics printed out after training and error-correcting. 
4. August 4, 2024: I have added extractions to the code, making it possible to visual the data. Specifically, I used t-SNE reduction to reduce the multi-dimensional data into a nice 2D plot. I also expand on my work in essay format, which can be found below.

**Project Essay:**

In this project, I dove into the world of Restricted Boltzmann Machines (RBMs) with guidance from David MacKay's Information Theory, Inference, and Learning Algorithms and Professor VanValkenburgh. My main goal was to leverage the theories from the book and build an effective model for unsupervised learning, especially focusing on genomic data. I started by processing data from a VCF file and converting it into a binary format that the RBM could handle. This involved picking out numeric data, filling in missing values, and binarizing everything to align with the RBM's requirement for binary inputs. This step was crucial for helping the RBM learn patterns effectively.

The RBM I built has visible and hidden units with weights initialized randomly and biases set to zero. At the core of the RBM is the idea of minimizing energy to learn dependencies between variables. The model works by reducing the energy of configurations that represent the data accurately and increasing it for those that don't. This energy-based approach allows the RBM to find complex patterns and dependencies within the data that are not immediately obvious.

I used the Hebbian learning rule, which was inspired by the neural network principles from MacKay's book, to update the weights based on the units' correlated activations. This strengthens connections between frequently co-activated units, embodying the principle that "neurons that fire together, wire together." The weight updates occur through the calculation of positive and negative phase interactions. The positive phase involves correlating the observed data with the hidden units, while the negative phase reconstructs the data from the hidden layer. The difference between these two phases updates the weights, guiding the RBM to capture the data's underlying structure.

The sigmoid function played a huge role here, helping me compute activation probabilities so the RBM could sample hidden and visible states. The sigmoid is crucial for introducing non-linearity into the model, which allows it to learn more complex relationships. It maps the weighted sum of inputs to a probability between 0 and 1, enabling probabilistic inference in the network. This capability is key to modeling the stochastic nature of genomic data, which often involves randomness and uncertainty.

After training, I introduced feature extraction and visualization techniques to get a better grasp of the learned representations. For feature extraction, I transformed input data into hidden layer activations, giving me a compact representation of the data. These features represent a compressed form of the original data, where each hidden unit activation reflects a specific pattern or trait that the RBM has learned.

To visualize these features, I turned to t-SNE, an ML tool for reducing dimensionality while preserving local structures. Mapping high-dimensional data into two dimensions is what t-SNE does. It allows us to visualize complex datasets and identify clusters, groups, or patterns. It also effectively captures the non-linear structures in the data, making the visualizations more insightful. The clusters and relationships within the data might be revealing potential genetic associations or variations of significance. However, I am not yet sure what patterns exist and what they might mean with regards to the genomic data. 

The visual makes the data and RBM more tangible, and I plan on continuing my analysis and visualization. For now, I interperet the clusters in the t-SNE plot as corresponding to distinct genomic profiles associated with different phenotypes or diseases. By identifying these clusters in the future, I can hypothesize about potential biomarkers or genetic signatures that could be explored further in a biological context.
